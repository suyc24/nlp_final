
import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../")))

from Universal_RAG.core import PrincipleRAGModel
from Universal_RAG.math_utils import MathEvaluator
from datasets import load_dataset
import random

def main():
    print("ğŸš€ [MATH Train] Starting...")
    
    # 1. å‡†å¤‡æ•°æ® (MATH Train)
    print("ğŸ“‚ Loading MATH dataset...")
    # æ³¨æ„ï¼šMATH æ•°æ®é›†é€šå¸¸è¾ƒå¤§ï¼Œä¸”åŠ è½½å¯èƒ½éœ€è¦ç‰¹å®šé…ç½®
    try:
        dataset = load_dataset("jeggers/competition_math", "original", split='train')
    except:
        print("âš ï¸ Failed to load jeggers/competition_math, trying lighteval/MATH...")
        dataset = load_dataset("lighteval/MATH", "all", split='train')

    # é‡‡æ · 200 æ¡ç”¨äºæ¼”ç¤º
    indices = list(range(len(dataset)))
    random.shuffle(indices)
    indices = indices[:200]
    
    training_data = []
    for i in indices:
        training_data.append({
            "question": dataset[i]['problem'],
            "ground_truth": dataset[i]['solution']
        })

    # 2. åˆå§‹åŒ–æ¨¡å‹
    model = PrincipleRAGModel(db_path="../math_notebook_db")

    # 3. å®šä¹‰ MATH ä¸“ç”¨çš„éªŒè¯å‡½æ•°
    evaluator = MathEvaluator()
    
    def math_verifier(pred_text, gt_text):
        # MathEvaluator.verify è¿”å› (bool, extracted_text)
        is_correct, _ = evaluator.verify(pred_text, gt_text)
        return is_correct

    # 4. å¼€å§‹è®­ç»ƒ
    print(f"ğŸ§  Training on {len(training_data)} samples with MathEvaluator...")
    model.train(training_data, verifier_func=math_verifier)
    print("âœ… Training finished!")

if __name__ == "__main__":
    main()
