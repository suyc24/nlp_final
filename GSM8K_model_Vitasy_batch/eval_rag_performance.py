import os
HF_CACHE_DIR = "/root/autodl-tmp/hf_cache"
os.makedirs(HF_CACHE_DIR, exist_ok=True)

os.environ["HF_HOME"] = HF_CACHE_DIR
os.environ["TRANSFORMERS_CACHE"] = HF_CACHE_DIR
os.environ["HUGGINGFACE_HUB_CACHE"] = HF_CACHE_DIR
# os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
import re
import json
import torch
import shutil
import numpy as np
import chromadb
from tqdm import tqdm
from collections import Counter
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
from vllm import LLM, SamplingParams
import config

# ================= é…ç½®åŒºåŸŸ =================
MODEL_PATH = config.EVAL_MODEL_PATH
DB_PATH = config.EVAL_DB_PATH
OUTPUT_FILE = config.OUTPUT_FILE

# RAG é…ç½®
TOP_K = config.TOP_K
SIMILARITY_THRESHOLD = config.SIMILARITY_THRESHOLD
BATCH_SIZE_EMBED = config.BATCH_SIZE_EMBED
SC_N = config.SC_N  # Self-Consistency é‡‡æ ·æ¬¡æ•°

# ================= 1. æ£€ç´¢å™¨ (CPU Mode) =================
class KnowledgeRetriever:
    def __init__(self, db_path):
        print(f"ğŸ“š åŠ è½½çŸ¥è¯†åº“: {db_path}...")
        self.client = chromadb.PersistentClient(path=db_path)
        self.collection = self.client.get_collection(name="elite_strategies")
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2', device="cpu")
        
    def batch_search(self, queries):
        # å¦‚æœæ²¡æœ‰æŸ¥è¯¢ï¼Œç›´æ¥è¿”å›
        if not queries: return []
        
        print(f"ğŸ” æ­£åœ¨æ£€ç´¢ {len(queries)} æ¡æŠ½è±¡åŒ–æŸ¥è¯¢...")
        q_embeddings = self.embedder.encode(
            queries, 
            batch_size=BATCH_SIZE_EMBED, 
            show_progress_bar=True, 
            convert_to_numpy=True
        ).tolist()
        
        search_results = self.collection.query(
            query_embeddings=q_embeddings,
            n_results=TOP_K
        )
        
        retrieved_contexts = []
        for i in range(len(queries)):
            valid_hints = []
            if search_results['ids'][i]:
                for j in range(len(search_results['ids'][i])):
                    distance = search_results['distances'][i][j]
                    doc_text = search_results['documents'][i][j]
                    metadata = search_results['metadatas'][i][j]
                    if distance < SIMILARITY_THRESHOLD:
                        valid_hints.append({
                            "strategy": doc_text,
                            "trigger": metadata.get('trigger', 'Unknown'),
                            "score": 1 - distance
                        })
            retrieved_contexts.append(valid_hints)
        return retrieved_contexts

# ================= 2. è¯„æµ‹å¼•æ“ (Adaptive SC) =================
class AdaptiveEvaluator:
    def __init__(self):
        print(f"ğŸš€ åˆå§‹åŒ–è¯„æµ‹å¼•æ“: {MODEL_PATH}")
        self.llm = LLM(
            model=MODEL_PATH,
            trust_remote_code=True,
            tensor_parallel_size=1,
            gpu_memory_utilization=0.90,
            max_model_len=4096,
            enforce_eager=True
        )
        
        # SC é‡‡æ ·å‚æ•°: Temperature > 0 æ‰èƒ½æœ‰å¤šæ ·æ€§
        self.params_sc = SamplingParams(
            n=SC_N,
            temperature=0.7, 
            max_tokens=1024,
            stop=["<|im_end|>", "<|endoftext|>"]
        )
        
        # RAG é‡‡æ ·å‚æ•°: T=0 æ±‚ç¨³
        self.params_rag = SamplingParams(
            temperature=0.2, 
            max_tokens=1024,
            stop=["<|im_end|>", "<|endoftext|>"]
        )
        
        # æŠ½è±¡åŒ–å‚æ•°
        self.params_abs = SamplingParams(temperature=0.0, max_tokens=128)

    # --- Prompt æ„é€ åŒº ---
    def construct_abstraction_prompt(self, q):
        return f"""<|im_start|>user
Task: Extract the underlying "Math Pattern" from the problem.
1. Remove specific numbers (replace with X, Y, etc.).
2. Remove entity names (e.g., "John" -> "Person", "Apples" -> "Items").
3. Describe the logical structure concisely.

[Example]
Input: John buys 5 apples for $2 each. Total?
Pattern: Calculating total cost given quantity and unit price.

[Target]
Input: {q}
Pattern:<|im_end|>
<|im_start|>assistant
"""

    def construct_base_prompt(self, q):
        return f"<|im_start|>user\nQuestion: {q}\nPlease reason step-by-step, and put your final answer within \\boxed{{}}.<|im_end|>\n<|im_start|>assistant\n"

    def construct_rag_prompt(self, q, hints):
        if not hints: return self.construct_base_prompt(q)
        
        strategies_text = ""
        for idx, h in enumerate(hints):
            strategies_text += f"Strategy {idx+1} (Matched Scenario: {h['trigger']}):\n{h['strategy']}\n\n"
        
        # --- ä¿®æ”¹å¼€å§‹ï¼šåŠ å…¥äº† [Demonstration] éƒ¨åˆ† ---
        content = f"""Reference Knowledge:
{strategies_text}
---
[Demonstration of how to use the Strategy]
Example Scenario:
Reference Strategy: "To find the total distance, multiply the speed by the time."
Question: A car travels at 60 mph for 3 hours. How far does it go?
Reasoning: The Reference Strategy suggests multiplying speed by time. 
Speed = 60, Time = 3. 
Calculation: 60 * 3 = 180.
The answer is \\boxed{{180}}.

---
[Your Turn]
Question: {q}
Instruction: First, check if any of the "Reference Knowledge" above applies to this question. If yes, explicitly use that logic. Reason step-by-step, and put your final answer within \\boxed{{}}."""
        # --- ä¿®æ”¹ç»“æŸ ---

        return f"<|im_start|>user\n{content}<|im_end|>\n<|im_start|>assistant\n"

    # --- å·¥å…·åŒº ---
    def extract_answer(self, text):
        if not text: return None
        text = text.replace(',', '')
        match = re.search(r'\\boxed\{(\-?\d+\.?\d*)\}', text)
        if match: return float(match.group(1))
        matches = re.findall(r'-?\d+\.?\d*', text[-100:])
        if matches: return float(matches[-1])
        return None

    def check_correct(self, pred, gt):
        if "####" in gt: gold = self.extract_answer(gt.split("####")[1])
        else: gold = self.extract_answer(gt)
        val = self.extract_answer(pred)
        if gold is None or val is None: return False
        return abs(gold - val) < 1e-4

    def get_majority_vote(self, outputs_list):
        """
        è¾“å…¥: ä¸€ä¸ª RequestOutput å¯¹è±¡ (åŒ…å« n ä¸ª completion)
        è¾“å‡º: (majority_answer, is_consistent)
        is_consistent = True è¡¨ç¤ºæœ‰ >=2 ä¸ªç­”æ¡ˆä¸€è‡´
        """
        answers = []
        for output in outputs_list.outputs:
            val = self.extract_answer(output.text)
            if val is not None:
                answers.append(val)
        
        if not answers: return None, False
        
        counts = Counter(answers)
        most_common_val, count = counts.most_common(1)[0]
        
        # SC=3 æ—¶ï¼Œå¦‚æœæœ‰2ä¸ªæˆ–3ä¸ªä¸€æ ·ï¼Œå°±ç®—ä¸€è‡´ï¼›å…¨æ˜¯1ä¸ªï¼Œåˆ™ä¸ä¸€è‡´
        is_consistent = (count >= 2)
        return most_common_val, is_consistent

    # --- ä¸»æµç¨‹ ---
    def run_evaluation(self):
        retriever = KnowledgeRetriever(DB_PATH)
        print("ğŸ“‚ åŠ è½½æµ‹è¯•é›† (GSM8K Test)...")
        dataset = load_dataset("gsm8k", "main")['test']
        questions = dataset['question']
        ground_truths = dataset['answer']
        
        # ==========================================
        # Stage 1: ç¬¬ä¸€æ¬¡å°è¯• - Self-Consistency (SC=3)
        # ==========================================
        print(f"\nâš¡ï¸ [Phase 1] è¿è¡Œ SC-3 æŠ•ç¥¨ (N={len(questions)})...")
        prompts_base = [self.construct_base_prompt(q) for q in questions]
        
        # è¿™é‡Œä¸€æ¬¡ç”Ÿæˆ n=3 ä¸ªå€™é€‰
        outputs_sc = self.llm.generate(prompts_base, self.params_sc)
        
        # åˆ†æ SC ç»“æœï¼Œç­›é€‰å‡ºéœ€è¦ RAG çš„é¢˜ç›®
        rag_indices = [] # éœ€è¦ RAG çš„é¢˜ç›®ç´¢å¼•
        rag_questions = [] # å¯¹åº”çš„æ–‡æœ¬
        final_results = [None] * len(questions) # é¢„å ä½
        
        consistent_count = 0
        baseline_correct = 0 # æ–°å¢ï¼šç»Ÿè®¡ Baseline æ­£ç¡®æ•°
        
        for i, output in enumerate(outputs_sc):
            # æŠ•ç¥¨
            maj_ans, is_consistent = self.get_majority_vote(output)
            
            # --- Baseline ç»Ÿè®¡ ---
            gt_text = ground_truths[i]
            if "####" in gt_text: gold = self.extract_answer(gt_text.split("####")[1])
            else: gold = self.extract_answer(gt_text)
            
            if maj_ans is not None and gold is not None and abs(maj_ans - gold) < 1e-4:
                baseline_correct += 1
                if not is_consistent:
                    print(f"\nğŸ² [Lucky Guess] ID: {i}")
                    print(f"Question: {questions[i]}")
                    print(f"Ground Truth: {gold}")
                    extracted_vals = [self.extract_answer(o.text) for o in output.outputs]
                    print(f"AI Answers (Extracted): {extracted_vals}")
            # --------------------

            if is_consistent:
                # æŠ•ç¥¨æˆåŠŸï¼Œç›´æ¥é‡‡çº³
                consistent_count += 1
                final_results[i] = {
                    "id": i,
                    "question": questions[i],
                    "ground_truth": ground_truths[i],
                    "method": "SC-3 (Consistent)",
                    "prediction": maj_ans,
                    "raw_output": output.outputs[0].text # å­˜ç¬¬ä¸€ä¸ªä½œä¸ºå‚è€ƒ
                }
            else:
                # æŠ•ç¥¨å¤±è´¥ï¼ˆ3ä¸ªç­”æ¡ˆéƒ½ä¸ä¸€æ ·ï¼‰ï¼Œè¿›å…¥ RAG é˜Ÿåˆ—
                rag_indices.append(i)
                rag_questions.append(questions[i])
        
        print(f"   -> ä¸€è‡´æ€§é€šè¿‡: {consistent_count}/{len(questions)}")
        print(f"   -> éœ€è¦ RAG ä»‹å…¥: {len(rag_questions)}/{len(questions)}")

        # ==========================================
        # Stage 2: é’ˆå¯¹ä¸ä¸€è‡´é¢˜ç›® - Adaptive RAG
        # ==========================================
        if rag_questions:
            print(f"\nğŸŒ€ [Phase 2.1] å¯¹ {len(rag_questions)} é“éš¾é¢˜è¿›è¡ŒæŠ½è±¡åŒ–...")
            # 2.1 æŠ½è±¡åŒ–
            abs_prompts = [self.construct_abstraction_prompt(q) for q in rag_questions]
            abs_outputs = self.llm.generate(abs_prompts, self.params_abs)
            abstract_queries = [o.outputs[0].text.strip() for o in abs_outputs]
            
            # 2.2 æ£€ç´¢
            print(f"\nğŸ” [Phase 2.2] æ£€ç´¢ç»éªŒ...")
            hints_list = retriever.batch_search(abstract_queries)
            
            # 2.3 RAG æ¨ç†
            print(f"\nâš¡ï¸ [Phase 2.3] è¿è¡Œ RAG æ¨ç† (Greedy)...")
            rag_prompts = [self.construct_rag_prompt(q, h) for q, h in zip(rag_questions, hints_list)]
            rag_outputs = self.llm.generate(rag_prompts, self.params_rag)
            
            # 2.4 å¡«å›ç»“æœ
            for idx, rag_idx in enumerate(rag_indices):
                output = rag_outputs[idx]
                pred = self.extract_answer(output.outputs[0].text)
                
                final_results[rag_idx] = {
                    "id": rag_idx,
                    "question": rag_questions[idx],
                    "ground_truth": ground_truths[rag_idx],
                    "method": "Adaptive RAG (Recovered)",
                    "prediction": pred,
                    "raw_output": output.outputs[0].text,
                    "retrieved_trigger": hints_list[idx][0]['trigger'] if hints_list[idx] else None
                }

        # ==========================================
        # Stage 3: ç»Ÿè®¡æœ€ç»ˆåˆ†æ•°
        # ==========================================
        print("\nğŸ“ˆ è®¡ç®—æœ€ç»ˆç»Ÿè®¡æ•°æ®...")
        correct_count = 0
        rag_wins = 0
        rag_total = len(rag_indices)
        
        # ç”¨æ¥åšå¯¹æ¯”ï¼šå¦‚æœå½“æ—¶SCå³ä½¿ä¸ä¸€è‡´ä¹Ÿå¼ºè¡Œé€‰ä¼—æ•°ä¼šæ€æ ·ï¼Ÿï¼ˆä½œä¸º Baseline å¯¹æ¯”ï¼‰
        # è¿™é‡Œä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬åªç»Ÿè®¡æœ€ç»ˆç³»ç»Ÿçš„å‡†ç¡®ç‡
        
        for res in final_results:
            gt = res['ground_truth']
            
            # æ£€æŸ¥æ­£ç¡®æ€§
            # æ³¨æ„ï¼šextract_answer å·²ç»åœ¨å‰é¢åšè¿‡äº†ï¼Œprediction æ˜¯ float æˆ– None
            # check_correct éœ€è¦é‡æ–°é€‚é…ä¸€ä¸‹å…¥å‚æ ¼å¼ï¼Œæˆ–è€…ç›´æ¥åœ¨è¿™é‡Œæ¯”å¯¹æ•°å­—
            
            if "####" in gt: gold_val = self.extract_answer(gt.split("####")[1])
            else: gold_val = self.extract_answer(gt)
            
            pred_val = res['prediction']
            
            is_right = False
            if gold_val is not None and pred_val is not None:
                if abs(gold_val - pred_val) < 1e-4:
                    is_right = True
            
            if is_right:
                correct_count += 1
                if "RAG" in res['method']:
                    rag_wins += 1
            
            res['is_correct'] = is_right

        acc = correct_count / len(questions) * 100
        baseline_acc = baseline_correct / len(questions) * 100
        rag_recovery_rate = (rag_wins / len(questions) * 100) if rag_total > 0 else 0
        
        print("\n" + "="*50)
        print("ğŸ† è‡ªé€‚åº” RAG è¯„æµ‹æŠ¥å‘Š")
        print("="*50)
        print(f"Total Questions      : {len(questions)}")
        print(f"Baseline Accuracy    : {baseline_acc:.2f}% (SC-3 Majority Vote)")
        print(f"Overall Accuracy     : {acc:.2f}%")
        print(f"RAG improved         : {rag_recovery_rate} %")
        print("-" * 50)
        print(f"RAG Activated        : {rag_total} cases")
        print(f"RAG Recovered (Win)  : {rag_wins} cases")
        print("="*50)
        
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“„ ç»“æœå·²ä¿å­˜è‡³: {OUTPUT_FILE}")

if __name__ == "__main__":
    try:
        import gc
        gc.collect()
        torch.cuda.empty_cache()
    except:
        pass

    evaluator = AdaptiveEvaluator()
    evaluator.run_evaluation()